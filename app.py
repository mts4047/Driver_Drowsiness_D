import streamlit as st
import cv2
import time
import numpy as np
import tensorflow as tf
import mediapipe as mp
from collections import deque

# Import modules
from config import *
from eye_utils import eye_aspect_ratio, LEFT_EYE_IDX, RIGHT_EYE_IDX
from mouth_utils import preprocess_mouth, get_mouth_roi
from alert import AlertSystem

# --- STREAMLIT UI SETUP ---
st.set_page_config(page_title="AI Driver Safety", layout="wide")
st.title("ðŸš— Driver Drowsiness Detection System")

# Use a sidebar for controls
st.sidebar.title("Settings")
run_app = st.sidebar.checkbox("Start/Stop Camera", value=False)
show_fps = st.sidebar.toggle("Show FPS", value=True)

# Placeholder for the video feed
frame_placeholder = st.empty()

# --- INITIALIZE MODELS ---
@st.cache_resource
def init_models():
    mouth_model = tf.keras.models.load_model(YAWN_MODEL_PATH)
    
    from mediapipe.tasks import python
    from mediapipe.tasks.python import vision
    base_options = python.BaseOptions(model_asset_path=FACE_LANDMARKER_PATH)
    options = vision.FaceLandmarkerOptions(
        base_options=base_options,
        running_mode=vision.RunningMode.VIDEO,
        num_faces=1
    )
    landmarker = vision.FaceLandmarker.create_from_options(options)
    
    alert_sys = AlertSystem(ALARM_SOUND_PATH)
    return mouth_model, landmarker, alert_sys

mouth_model, face_landmarker, alert = init_models()

# Initialize history outside the loop
ear_history = deque(maxlen=EAR_HISTORY)
mouth_history = deque(maxlen=MOUTH_HISTORY)
eye_closed_start = None
prev_time = time.time()

# --- STREAMING LOOP ---
cap = cv2.VideoCapture(0)

while run_app:
    ret, frame = cap.read()
    if not ret:
        st.error("Cannot access camera.")
        break

    h, w, _ = frame.shape
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)

    # MediaPipe Detection
    result = face_landmarker.detect_for_video(mp_image, int(time.time() * 1000))

    eye_state, eye_color = "AWAKE", (0, 255, 0)
    mouth_state, mouth_color = "NOT YAWNING", (0, 255, 0)

    if result.face_landmarks:
        landmarks = result.face_landmarks[0]

        # 1. EAR Calculation
        left_eye = [landmarks[i] for i in LEFT_EYE_IDX]
        right_eye = [landmarks[i] for i in RIGHT_EYE_IDX]
        ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0
        ear_history.append(ear)
        smooth_ear = np.mean(ear_history)

        eye_closed = smooth_ear < EAR_THRESHOLD
        if eye_closed:
            eye_state, eye_color = "SLEEPY", (255, 0, 0) # Streamlit uses RGB
        
        # 2. Mouth Detection
        mouth_roi, mouth_box = get_mouth_roi(frame, landmarks, w, h)
        mouth_input = preprocess_mouth(mouth_roi)

        if mouth_input is not None:
            # We use [0][1] because model is Categorical (No_Yawn at 0, Yawn at 1)
            pred = mouth_model.predict(mouth_input, verbose=0)[0][1]
            mouth_history.append(pred)

        smooth_mouth = np.mean(mouth_history) if mouth_history else 0
        if smooth_mouth > YAWN_THRESHOLD:
            mouth_state, mouth_color = "YAWNING", (255, 0, 0)

        # 3. Drowsiness Logic & Alerts
        now = time.time()
        if eye_closed:
            if eye_closed_start is None:
                eye_closed_start = now
            elif now - eye_closed_start > CLOSED_EYE_SECONDS:
                cv2.rectangle(frame, (0, 0), (w, 80), (0, 0, 255), -1)
                cv2.putText(frame, "DROWSINESS DETECTED!", (120, 55), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
                alert.play()
        else:
            eye_closed_start = None

        # Draw Visuals (Convert frame back to RGB for Streamlit display)
        if mouth_box:
            x1, y1, x2, y2 = mouth_box
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 3)

    # 4. Final Frame prep
    if show_fps:
        fps = int(1 / (time.time() - prev_time))
        prev_time = time.time()
        cv2.putText(frame, f"FPS: {fps}", (w-150, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

    cv2.putText(frame, eye_state, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.9, eye_color, 2)
    cv2.putText(frame, mouth_state, (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.9, mouth_color, 2)

    # Streamlit Display
    # Convert BGR (OpenCV) to RGB (Streamlit)
    output_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame_placeholder.image(output_frame, channels="RGB")

# Cleanup
cap.release()